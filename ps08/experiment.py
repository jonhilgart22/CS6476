import os

from ps8 import *


scaled_moment_features = None
central_moment_features = None


# Driver/helper code
def build_motion_history_image(builder_class, video_filename, save_frames={}, mhi_frame=None, mhi_filename=None,
                               **kwargs):
    """Instantiates and run a motion history builder on a given video, return MHI.

    Do not modify this function. You may uncomment the debug line when you are working on your assignment. Make sure
    you comment it out before submitting.

    Args:
        builder_class (object): motion history builder class to instantiate.
        video_filename (str): path to input video file.
        save_frames (dict): output binary motion images to save {<frame number>: <filename>}.
        mhi_frame (int): which frame to obtain the motion history image at.
        mhi_filename (str): output filename to save motion history image.
        **kwargs: arbitrary keyword arguments passed on to constructor.

    Returns:
        numpy.array: float motion history image generated by builder, values in [0.0, 1.0].
    """

    # Open video file
    video = cv2.VideoCapture(video_filename)
    print("Video: {} ({}x{}, {:.2f} fps, {} frames)".format(
        video_filename,
        int(video.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)),
        int(video.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)),
        video.get(cv2.cv.CV_CAP_PROP_FPS),
        int(video.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT))))

    # Initialize objects
    mhi_builder = None
    mhi = None
    frame_num = 0

    # Loop over video (till last frame or Ctrl+C is presssed)
    while True:
        try:
            # Try to read a frame
            okay, frame = video.read()
            if not okay:
                break  # no more frames, or can't read video

            # Initialize motion history builder (one-time only)
            if mhi_builder is None:
                mhi_builder = builder_class(frame, **kwargs)

            # Process frame
            motion_image = mhi_builder.process(frame)

            if False:  # For debugging, it shows every frame
                out_frame = motion_image.copy()
                cv2.imshow('frame', 1. * out_frame)
                cv2.waitKey(1)  # Set to 0 if you want to continue by pressing any key

            # Save output, if indicated
            if frame_num in save_frames:
                cv2.imwrite(save_frames[frame_num], np.uint8(motion_image * 255))  # scale [0, 1] => [0, 255]

            # Grab MHI, if indicated
            if frame_num == mhi_frame:
                mhi = mhi_builder.get_mhi()
                print("MHI frame: {}".format(mhi_frame))
                break  # uncomment for early stop

            # Update frame number
            frame_num += 1
        except KeyboardInterrupt:  # press ^C to quit
            break

    # If not obtained earlier, get MHI now
    if mhi is None:
        mhi = mhi_builder.get_mhi()

    # Save MHI, if filename is given
    if mhi_filename is not None:
        cv2.imwrite(mhi_filename, np.uint8(mhi * 255))  # scale [0, 1] => [0, 255]

    return mhi


def get_cs_moment_features(n_actions, n_participants, n_trials, default_params, custom_params):
    """Computes the central and scale features for each video.

    Do not modify this function. You may uncomment the debug line when you are working on your assignment. Make sure
    you comment it out before submitting.

    Args:
        n_actions (int): number of actions, in this problem set: 3.
        n_participants (int): number of participants, in this problem set: 3.
        n_trials (int): number of trials, in this problem set: 3.
        default_params (dict): default values for parameters in the MHI Builder.
        custom_params (dict): custom parameters for specific video files to be used in the MHI Builder.

    Returns:
        tuple: 2-element tuple containing:
            c_moment_features (dict): 16 central momment features (8 MHI, 8 MEI) as one vector, key:
                                            (<action>, <participant>, <trial>).
            s_moment_features (dict): 16 scale moment features (8 MHI, 8 MEI) as one vector, key:
                                            (<action>, <participant>, <trial>).
    """
    c_moment_features = {}  # 16 features (8 MHI, 8 MEI) as one vector, key: (<action>, <participant>, <trial>)
    s_moment_features = {}  # similarly, scaled central moments

    # Loop for each action, participant, trial
    print("Computing features for each video...")
    for a in range(1, n_actions + 1):  # actions
        for p in range(1, n_participants + 1):  # participants
            for t in range(1, n_trials + 1):  # trials
                video_filename = os.path.join(input_dir, "PS8A{}P{}T{}.mp4".format(a, p, t))
                mhi = build_motion_history_image(MotionHistoryBuilder, video_filename,
                                                 **dict(default_params, **custom_params.get((a, p, t), {})))

                # cv2.imshow("MHI: PS8A{}P{}T{}.mp4".format(a, p, t), mhi)  # [debug]
                # cv2.waitKey(1)  # uncomment if using imshow

                mei = np.uint8(mhi > 0)
                mhi_moments = Moments(mhi)
                mei_moments = Moments(mei)
                c_moment_features[(a, p, t)] = np.hstack(
                    (mhi_moments.get_central_moments(), mei_moments.get_central_moments()))
                s_moment_features[(a, p, t)] = np.hstack(
                    (mhi_moments.get_scaled_moments(), mei_moments.get_scaled_moments()))

    return c_moment_features, s_moment_features


def match_features(a_features_dict, b_features_dict, n_actions, scale=0.5):
    """Compares features, tally matches for each action pair to produce a confusion matrix.

    Do not modify this function. You may uncomment the debug line when you are working on your assignment. Make sure
    you comment it out before submitting.

    Args:
        a_features_dict (dict): one set of features, as a dict with key: (<action>, <participant>, <trial>).
        b_features_dict (dict): another set of features like a_features.
        n_actions (int): number of distinct actions present in the feature sets.
        scale (float): scale factor for compute_feature_difference (if needed).

    Returns:
        numpy.array: table of matches found, n_actions by n_actions.
    """

    confusion_matrix = np.zeros((n_actions, n_actions), dtype=np.float_)
    for a_key, a_features in a_features_dict.items():
        min_diff = np.inf
        best_match = None
        for b_key, b_features in b_features_dict.items():
            if a_key == b_key:
                continue  # don't compare with yourself!
            diff = compute_feature_difference(a_features, b_features, scale)
            if diff < min_diff:
                min_diff = diff
                best_match = b_key
        if best_match is not None:
            # print("{} matches {}, diff: {}".format(a_key, best_match, min_diff))  # [debug]
            confusion_matrix[a_key[0] - 1, best_match[0] - 1] += 1  # note: 1-based to 0-based indexing

    confusion_matrix /= confusion_matrix.sum(axis=1)[:, np.newaxis]  # normalize confusion_matrix along each row
    return confusion_matrix


def part_1a():
    filename = "PS8A1P1T1.mp4"  # Video file that represents Action 1, Person 1, Trial 1
    theta = 0.  # Define a value for theta
    build_motion_history_image(MotionHistoryBuilder,  # motion history builder class
                               os.path.join(input_dir, filename),  # input video
                               save_frames={
                                   10: os.path.join(output_dir, 'ps8-1-a-1.png'),
                                   20: os.path.join(output_dir, 'ps8-1-a-2.png'),
                                   30: os.path.join(output_dir, 'ps8-1-a-3.png')
                               },
                               theta=theta)  # Specify any other keyword args that your motion history builder
                                             # expects, e.g. theta, tau


def part_1b_1():
    filename = "PS8A1P1T1.mp4"  # Use Action 1. You may choose a different Person and Trial.
    mhi_frame = 90  # Pick a good frame to obtain MHI at, i.e. when action just ends
    theta = 0.  # Define a value for theta
    tau = 0.  # Define a value for theta
    build_motion_history_image(MotionHistoryBuilder,  # motion history builder class
                               os.path.join(input_dir, filename),
                               mhi_frame=mhi_frame,
                               mhi_filename=os.path.join(output_dir, 'ps8-1-b-1.png'),
                               theta=theta,
                               tau=tau
                               )  # Specify any other keyword args that your motion history builder
                                  # expects, e.g. theta, tau


def part_1b_2():
    """Performs a similar procedure used in part_1b_1 using actions 2 and 3.

    In this method you will select the videos that contain Actions 2 and 3. And save the images:
    - MHI image for action A2 as ps8-1-b-2.png
    - MHI image for action A3 as ps8-1-b-3.png

    Returns:
        None
    """

    # TODO: Your code here
    pass


def part_2a():
    global scaled_moment_features
    global central_moment_features

    # Compute MHI and MEI features (unscaled and scaled central moments) for each video

    # Parameters for build_motion_history(), overridden by custom_params for specified videos
    default_params = dict(mhi_frame=60)

    # Note: To specify custom parameters for a video, add to the dict below:
    #   (<action>, <participant>, <trial>): dict(<param1>=<value1>, <param2>=<value2>, ...)
    custom_params = {
        (1, 1, 3): dict(mhi_frame=90),  # PS8A1P1T3.mp4 Reference value you may use a different one
        (1, 2, 3): dict(mhi_frame=55)  # PS8A1P2T3.mp4 Reference value you may use a different one
        # You can add more if needed up to one for each video following the format:
        # (1, 3, 4): dict(mhi_frame=value1, theta=value2, tau=value3)
    }

    n_actions = 3
    n_participants = 3
    n_trials = 3

    central_moment_features, scaled_moment_features = get_cs_moment_features(n_actions, n_participants, n_trials,
                                                                             default_params, custom_params)

    # Match features in a leave-one-out scheme (each video with all others)
    central_moments_confusion = match_features(central_moment_features, central_moment_features, n_actions)
    print("Confusion matrix (unscaled central moments):-")
    print(central_moments_confusion)

    # Similarly with scaled moments
    scaled_moments_confusion = match_features(scaled_moment_features, scaled_moment_features, n_actions)
    print("Confusion matrix (scaled central moments):-")
    print(scaled_moments_confusion)


def part_2b():

    if scaled_moment_features is None or central_moment_features is None:
        raise ValueError("Part 2a has not modified the moment features. Run part_2a before part_2b")

    # Match features by testing one participant at a time (i.e. taking them out)
    # Note: Pick one between central_moment_features and scaled_moment_features

    person = 1
    n_actions = 3
    scale = 0.5  # Select a different scale factor (if needed) to help obtain an identity confusion matrix
    features = central_moment_features  # Pick one between central_moment_features and scaled_moment_features
    features_p1 = {key: feature for key, feature in features.items() if key[1] == person}
    features_sans_p1 = {key: feature for key, feature in features.items() if key[1] != person}
    confusion_p1 = match_features(features_p1, features_sans_p1, n_actions, scale)
    print("Confusion matrix for P{}:-".format(person))
    print(confusion_p1)

    # TODO: Similarly for participants P2 & P3
    # TODO: Finally find the Average confusion matrix of P1, P2, and P3

if __name__ == "__main__":
    part_1a()
    part_1b_1()
    part_1b_2()
    part_2a()
    part_2b()
